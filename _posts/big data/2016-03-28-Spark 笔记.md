---
layout: post
title:  "Spark 笔记"
date:   2016-08-26
categories: BigData
excerpt: 
tags: Spark BigData
---

* content
{:toc}

## Why not hadoop

### Why data sharing in Hadoop is slow

Both Iterative and Interactive applications require faster data sharing across parallel jobs. Data sharing is slow in MapReduce due to replication, serialization, and disk IO. Regarding storage system, most of the Hadoop applications, they spend more than 90% of the time doing HDFS read-write operations.

## Motivation of Spark

* **Iterative jobs**: machine learning algorithms
apply a function repeatedly to the same dataset
to optimize a parameter
* **Interactive analytics**: a user would be able to load a dataset of interest into
memory across a number of machines and query it repeatedly.


## RDD

RDD is data abstract, it does not keep actual data, but keep a series of transformations, so it is good for fault tolerance. 
Once fail, it only need to recover the failed task by doing the series of transformations.

The transform in spark will not trigger actual computation, only action will.

```
val transformedRDD = rddFromTextFile.map(line => line.size).
filter(size => size > 10).map(size => size * 2)
```
the above code is transform only and will only return a series of new RDD with the transform operation link.

if we call sum,
```
val computation = transformedRDD.sum
```
then the computation will be performed.

If we will use data multiple times, then we can call cache to cache the data into memory

```
// (user, product, price)
val data = sc.textFile("data/UserPurchaseHistory.csv")
  .map(line => line.split(","))
  .map(purchaseRecord => (pr(0), pr(1), pr(2))

val numPurchases = data.count()
val UniqueUsers = data.map{case (user, product, price) => user}.distinct().count()
val totalRevenue = data.map{case (user, product, price) => price.toDouble}.sum()
val productByPopularity = data.map{ case (user, product, price) => (product, 1) }
  .reduceByKey(_ + _)
  .collect()
  .sortBy(-_._2)

```
the collect will trigger computation and return results to locally

## RDD vs DataFrame vs Dataset

* DataFrame and Datasets are built on top of RDDs, they have schemas and optimise operations for data with schemas
* In spark 2.0, Datasets and DataFrame will merge as Dataset API
* But RDD is not second-class citizen, it is still useful when your data has no schema, or you want low-level transformation.

## relationship between SparkContext, SparkConf and SQLContent
[relationship between SparkContext, SparkConf and SQLContent](https://blogs.msdn.microsoft.com/bigdatasupport/2015/09/14/understanding-sparks-sparkconf-sparkcontext-sqlcontext-and-hivecontext/) 

very clear explanation

## Reference
http://www.tutorialspoint.com/apache_spark/apache_spark_introduction.htm
