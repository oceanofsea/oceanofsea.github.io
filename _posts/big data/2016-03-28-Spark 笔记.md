---
layout: post
title:  "Spark 笔记"
date:   2016-08-26
categories: BigData
excerpt: 
tags: Spark BigData
---

* content
{:toc}

## Why not hadoop

### Why data sharing in Hadoop is slow

Both Iterative and Interactive applications require faster data sharing across parallel jobs. Data sharing is slow in MapReduce due to replication, serialization, and disk IO. Regarding storage system, most of the Hadoop applications, they spend more than 90% of the time doing HDFS read-write operations.

## Motivation of Spark

* **Iterative jobs**: machine learning algorithms
apply a function repeatedly to the same dataset
to optimize a parameter
* **Interactive analytics**: a user would be able to load a dataset of interest into
memory across a number of machines and query it repeatedly.


## RDD

RDD is data abstract, it does not keep actual data, but keep a series of transformations, so it is good for fault tolerance. 
Once fail, it only need to recover the failed task by doing the series of transformations.

The transform in spark will not trigger actual computation, only action will.

```
val transformedRDD = rddFromTextFile.map(line => line.size).
filter(size => size > 10).map(size => size * 2)
```
the above code is transform only and will only return a series of new RDD with the transform operation link.

if we call sum,
```
val computation = transformedRDD.sum
```
then the computation will be performed.

If we will use data multiple times, then we can call cache to cache the data into memory

```
// (user, product, price)
val data = sc.textFile("data/UserPurchaseHistory.csv")
  .map(line => line.split(","))
  .map(purchaseRecord => (pr(0), pr(1), pr(2))

val numPurchases = data.count()
val UniqueUsers = data.map{case (user, product, price) => user}.distinct().count()
val totalRevenue = data.map{case (user, product, price) => price.toDouble}.sum()
val productByPopularity = data.map{ case (user, product, price) => (product, 1) }
  .reduceByKey(_ + _)
  .collect()
  .sortBy(-_._2)

```
the collect will trigger computation and return results to locally

## RDD vs DataFrame vs Dataset

* DataFrame and Datasets are built on top of RDDs, they have schemas and optimise operations for data with schemas
* In spark 2.0, Datasets and DataFrame will merge as Dataset API
* But RDD is not second-class citizen, it is still useful when your data has no schema, or you want low-level transformation.

## relationship between SparkContext, SparkConf and SQLContent
[relationship between SparkContext, SparkConf and SQLContent](https://blogs.msdn.microsoft.com/bigdatasupport/2015/09/14/understanding-sparks-sparkconf-sparkcontext-sqlcontext-and-hivecontext/) 

very clear explanation

## Spark Execution(https://www.safaribooksonline.com/library/view/data-analytics-with/9781491913734/ch04.html)

A typical data flow sequence for programming Spark is as follows:

Define one or more RDDs, either through accessing data stored on disk (e.g., HDFS, HBase), parallelizing some collection, transforming an existing RDD, or by caching. 

Invoke operations on the RDD by passing closures to each element of the RDD. Spark offers many high-level operators beyond map and reduce.

Use the resulting RDDs with aggregating actions (e.g., count, collect, save, etc.). 
Actions kick off the computation on the cluster because no progress can be made until the aggregation has been computed.

When Spark runs a closure on a worker, any variables used in the closure are copied to that node, 
but are maintained within the local scope of that closure. 

If external data is required, you can use broadcast variables and accumulators. 

Essentially, Spark applications are run as independent sets of processes, coordinated by a SparkContext in a driver program. 
The context will connect to some cluster manager (e.g., YARN), which allocates system resources. Each worker in the cluster is managed by an executor, 
which is in turn managed by the SparkContext. The executor manages computation as well as storage and caching on each machine. 
The interaction of the driver, YARN, and the workers is shown in Figure 4-3.

It is important to note that application code is sent from the driver to the executors, 
and the executors specify the context and the various tasks to be run. 
The executors communicate back and forth with the driver for data sharing or for interaction. 
Drivers are key participants in Spark jobs, and therefore, they should be on the same network as the cluster. 
This is different from Hadoop code, where you might submit a job from anywhere to the ResourceManager, which then handles the execution on the cluster.

![spark on yarn](https://www.safaribooksonline.com/library/view/data-analytics-with/9781491913734/assets/dawh_0403.png)

## Reference
http://www.tutorialspoint.com/apache_spark/apache_spark_introduction.htm
