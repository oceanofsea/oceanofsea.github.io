---
layout: post
title:  "Spark QA"
date:   2016-08-23
categories: BigData
excerpt: 
tags: Spark Useful QA
---

* content
{:toc}

## log4j

Spark loads its default log4j properties, and some information is hidden.

To change this, you can define your own [log4j.properties](https://gist.github.com/SeaOfOcean/d7e7e43411801114072f4e7edbc3c8d1)

And the "standard placement" for this is 

```
src/main/resources
src/test/resources
```

The folder structure is as follows:

![log4j folder structure](/images/posts/log4j.png)

Now it works!

## Submit spark job

First you need to build your spark source by running 

```
./build/mvn -DskipTests clean package
```

Then you can set your SPARK_HOME in .bashrc for easy access

```
export SPARK_HOME=$HOME/Downloads/spark-1.6.2
export PATH=$PATH:$SPARK_HOME/bin
```

Then you can package your project using maven. Similar to spark package.
The packaged jar can be found in ```target``` folder.

To submit the spark application, you can reference [this guide](http://spark.apache.org/docs/latest/submitting-applications.html#submitting-applications).

For reusable purpose, you can write a script like 

```
#!/bin/bash

PARK_HOME=/opt/spark-1.6.2-bin-hadoop2.6
TargetJar=target/ie-project-1.0-SNAPSHOT-jar-with-dependencies.jar
MainClass=com.intel.ie.SparkBatchDriver


$SPARK_HOME/bin/spark-submit \
  --master yarn \
  --driver-memory 20g \
  --executor-memory 20g \
  --num-executors 4 \
  --class $MainClass \
  --jars lib/stanford-english-corenlp-models-current.jar,intel-resources.jar \
  --files config.properties \
  $TargetJar 8

```
Note that if some resources are needed by all workers, you need to pass them in submit,
otherwise they cannot find these files.
One good way is to package the folders into a jar by using command ```jar cf intel-resources.jar model data```.
Then you can easily pass the jar after --jars, separated by comma.

you can also pass files after --files, this is very useful if you have any config files.

Note that by default all dependency jars will be packaged into single fat jar, 
except those system jars or provided jars. The reason is explained in [this post]({% post_url/useful stuff/2016-08-23-maven %})

In this way, you can run your spark application locally.

And other type may be similar and I'll try these days.

Note that the driver-memory need to be set, otherwise will easily get out of memory error.


You can easily have multiple sparks by using different spark_home :)


